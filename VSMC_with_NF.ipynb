{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eecf4357-b684-4f60-8001-9885067f75dc",
   "metadata": {},
   "source": [
    "# VSMC with Normalizing Flows\n",
    "\n",
    "### To do:\n",
    "- [ ] Think about how to take into consideration `x_prev`\n",
    "- [ ] Implement it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e323981-52d3-4599-8589-cab8d2769852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import normalizing flows module\n",
    "import sys\n",
    "sys.path.append(\"./src\")\n",
    "from flows import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c94534f0-3f38-4202-9eb0-9a5996eec067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import other libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import distributions\n",
    "from torch.nn.parameter import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "40f3237a-0766-4174-ab7b-b7a51430de15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward pass\n",
      "torch.Size([5, 2]) \n",
      " torch.Size([5, 2])\n",
      "torch.Size([5, 2]) \n",
      " torch.Size([5, 2])\n",
      "torch.Size([5, 2]) \n",
      " torch.Size([5, 2])\n",
      "torch.Size([5, 2]) \n",
      " torch.Size([5, 2])\n",
      "inverse pass\n",
      "log prob\n",
      "sample\n",
      "torch.Size([10, 2]) \n",
      " torch.Size([5, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 10 but got size 5 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 139\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Sampling from the model\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 139\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43mrealnvp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 107\u001b[0m, in \u001b[0;36mRealNVP.sample\u001b[0;34m(self, n_samples, context)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, n_samples, context):\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# Sample from the final distribution\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_dist\u001b[38;5;241m.\u001b[39msample((n_samples,))\n\u001b[0;32m--> 107\u001b[0m     x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "Cell \u001b[0;32mIn[35], line 87\u001b[0m, in \u001b[0;36mRealNVP.forward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m     85\u001b[0m log_det_J \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 87\u001b[0m     x, log_det \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     log_det_J \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m log_det\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, log_det_J\n",
      "Cell \u001b[0;32mIn[35], line 46\u001b[0m, in \u001b[0;36mRealNVPLayer.forward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Adding context\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(x_masked\u001b[38;5;241m.\u001b[39mshape,\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,context\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 46\u001b[0m x_masked_with_context \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_masked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_net(x_masked_with_context) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask)\n\u001b[1;32m     48\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranslate_net(x_masked_with_context) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 10 but got size 5 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal, MultivariateNormal\n",
    "\n",
    "# Classes and functions necessary for Real NVP\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    Multilayer perceptron module.\n",
    "    '''\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class RealNVPLayer(nn.Module):\n",
    "    '''\n",
    "    Real NVP layer module.\n",
    "    '''\n",
    "    def __init__(self, dim, mask, context_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Uses MLP module as scale and\n",
    "        # translate neural networks\n",
    "        self.scale_net = MLP(dim + context_dim, dim)\n",
    "        self.translate_net = MLP(dim + context_dim, dim)\n",
    "\n",
    "        # Defines mask to implement\n",
    "        # coupling layers\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        # Forward transformation\n",
    "        x_masked = x * self.mask\n",
    "        # Adding context\n",
    "        print(x_masked.shape,'\\n',context.shape)\n",
    "        x_masked_with_context = torch.cat([x_masked, context], dim=-1)\n",
    "        s = self.scale_net(x_masked_with_context) * (1 - self.mask)\n",
    "        t = self.translate_net(x_masked_with_context) * (1 - self.mask)\n",
    "        z = x_masked + (1 - self.mask) * (x * torch.exp(s) + t)\n",
    "        log_det_J = torch.sum(s, dim=1)\n",
    "        return z, log_det_J\n",
    "\n",
    "    def inverse(self, y, context):\n",
    "        # Inverse transformation\n",
    "        y_masked = y * self.mask\n",
    "        # Adding context\n",
    "        y_masked_with_context = torch.cat([y_masked, context], dim=-1)\n",
    "        s = self.scale_net(y_masked_with_context) * (1 - self.mask)\n",
    "        t = self.translate_net(y_masked_with_context) * (1 - self.mask)\n",
    "        x = y_masked + (1 - self.mask) * ((y - t) * torch.exp(-s))\n",
    "        log_det_J = torch.sum(s, dim=1)\n",
    "        return x, log_det_J\n",
    "\n",
    "\n",
    "class RealNVP(nn.Module):\n",
    "    '''\n",
    "    Real NVP module.\n",
    "    Consists of multiple Real NVP layers.\n",
    "    '''\n",
    "    def __init__(self, dim, n_layers, base_dist, context_dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.base_dist = base_dist\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(n_layers):\n",
    "            # Create masks (alternating 0s and 1s)\n",
    "            mask_list = [(i + j) % 2 for j in range(dim)]\n",
    "            mask = torch.tensor(mask_list, dtype=torch.float32)\n",
    "\n",
    "            # Add Real NVP layer\n",
    "            self.layers.append(RealNVPLayer(self.dim, mask, context_dim))\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        # Forward transformation\n",
    "        log_det_J = torch.zeros(x.size(0), device=x.device)\n",
    "        for layer in self.layers:\n",
    "            x, log_det = layer.forward(x, context)\n",
    "            log_det_J += log_det\n",
    "        return x, log_det_J\n",
    "\n",
    "    def inverse(self, z, context):\n",
    "        # Inverse transformation\n",
    "        log_det_J = torch.zeros(z.size(0), device=z.device)\n",
    "        for layer in reversed(self.layers):\n",
    "            z, log_det = layer.inverse(z, context)\n",
    "            log_det_J -= log_det\n",
    "        return z, log_det_J\n",
    "\n",
    "    def log_prob(self, x, context):\n",
    "        # Computes the log pdf of the final samples\n",
    "        z, log_det = self.inverse(x, context)\n",
    "        return self.base_dist.log_prob(z) + log_det\n",
    "        \n",
    "    def sample(self, n_samples, context):\n",
    "        # Sample from the final distribution\n",
    "        z = self.base_dist.sample((n_samples,))\n",
    "        x, _ = self.forward(z.view(n_samples, self.dim), context)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define a base distribution\n",
    "base_dist = MultivariateNormal(torch.zeros(2), torch.eye(2))\n",
    "\n",
    "# Example usage\n",
    "dim = 2  # Dimensionality of data\n",
    "n_layers = 4  # Number of RealNVP layers\n",
    "context_dim = 2  # Dimensionality of context\n",
    "\n",
    "realnvp = RealNVP(dim, n_layers, base_dist, context_dim)\n",
    "\n",
    "# Sample data\n",
    "context = torch.randn(5, context_dim)  # 10 samples, 2-dimensional context\n",
    "x = torch.randn(5, dim)  # 10 samples, 3-dimensional data\n",
    "\n",
    "# Forward pass\n",
    "print('forward pass')\n",
    "z, log_det_J = realnvp(x, context)\n",
    "\n",
    "# Inverse pass\n",
    "print('inverse pass')\n",
    "x_reconstructed, _ = realnvp.inverse(z, context)\n",
    "\n",
    "# Log probability\n",
    "print('log prob')\n",
    "log_prob = realnvp.log_prob(x, context)\n",
    "\n",
    "# Sampling from the model\n",
    "print('sample')\n",
    "samples = realnvp.sample(10, context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1da93ff4-5892-4aaf-95ea-145f9c01ddd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.0718, -2.9634, -1.9189, -2.6762, -5.1024])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultivariateNormal(torch.zeros(2), torch.eye(2)).log_prob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af45e5ef-94e0-4bce-8675-69887e451dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4748, -0.8736])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b16e5c5-2d6b-4363-b491-9356c32afe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = distributions.Normal(0, 1)\n",
    "flow = RealNVP(dim=2, n_layers=5, base_dist=normal, context_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fddca5c-0ae7-459b-b50c-a1f051ab663f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 2.1000]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = torch.tensor([[1.0, 2.1]])\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52b09292-e6a0-4a46-9a18-d27bcf36b082",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Not trained yet\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Estudos/Mestrado/Estatística Computacional/Final Project/ComputacionalStatistics_FinalProject/./src/flows.py:104\u001b[0m, in \u001b[0;36mRealNVP.log_prob\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, context):\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# Computes the log pdf of the final samples\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m     z, log_det \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_dist\u001b[38;5;241m.\u001b[39mlog_prob(z) \u001b[38;5;241m+\u001b[39m log_det\n",
      "File \u001b[0;32m~/Desktop/Estudos/Mestrado/Estatística Computacional/Final Project/ComputacionalStatistics_FinalProject/./src/flows.py:98\u001b[0m, in \u001b[0;36mRealNVP.inverse\u001b[0;34m(self, z, context)\u001b[0m\n\u001b[1;32m     96\u001b[0m log_det_J \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(z\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m---> 98\u001b[0m     z, log_det \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     log_det_J \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m log_det\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m z, log_det_J\n",
      "File \u001b[0;32m~/Desktop/Estudos/Mestrado/Estatística Computacional/Final Project/ComputacionalStatistics_FinalProject/./src/flows.py:59\u001b[0m, in \u001b[0;36mRealNVPLayer.inverse\u001b[0;34m(self, y, context)\u001b[0m\n\u001b[1;32m     57\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_net(y_masked_with_context) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask)\n\u001b[1;32m     58\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranslate_net(y_masked_with_context) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask)\n\u001b[0;32m---> 59\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43my_masked_with_context\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m log_det_J \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(s, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, log_det_J\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Not trained yet\n",
    "flow.log_prob(torch.tensor([[0.0]]),context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78f5ba02-cffe-4f1b-8df5-81b590c04517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to fix it (by gepeto)\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# ---------------------\n",
    "# Helper MLP class\n",
    "# ---------------------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "# RealNVP Layer\n",
    "# ---------------------\n",
    "class RealNVPLayer(nn.Module):\n",
    "    def __init__(self, dim, mask, context_dim):\n",
    "        super().__init__()\n",
    "        self.scale_net = MLP(dim + context_dim, dim)\n",
    "        self.translate_net = MLP(dim + context_dim, dim)\n",
    "        self.mask = nn.Parameter(mask, requires_grad=False)\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        x_masked = x * self.mask\n",
    "        print(x_masked.size(), context.size())\n",
    "        x_input = torch.cat([x_masked, context], dim=-1)\n",
    "        s = self.scale_net(x_input) * (1 - self.mask)\n",
    "        t = self.translate_net(x_input) * (1 - self.mask)\n",
    "        z = x_masked + (1 - self.mask) * (x * torch.exp(s) + t)\n",
    "        log_det_J = torch.sum(s, dim=1)\n",
    "        return z, log_det_J\n",
    "\n",
    "    def inverse(self, y, context):\n",
    "        y_masked = y * self.mask\n",
    "        y_input = torch.cat([y_masked, context], dim=-1)\n",
    "        s = self.scale_net(y_input) * (1 - self.mask)\n",
    "        t = self.translate_net(y_input) * (1 - self.mask)\n",
    "        x = y_masked + (1 - self.mask) * ((y - t) * torch.exp(-s))\n",
    "        log_det_J = torch.sum(s, dim=1)\n",
    "        return x, log_det_J\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "# RealNVP Model\n",
    "# ---------------------\n",
    "class RealNVP(nn.Module):\n",
    "    def __init__(self, dim, n_layers, base_dist, context_dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.base_dist = base_dist\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            mask = torch.tensor([(i + j) % 2 for j in range(dim)], dtype=torch.float32)\n",
    "            self.layers.append(RealNVPLayer(dim, mask, context_dim))\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        log_det_J = torch.zeros(x.size(0), device=x.device)\n",
    "        for layer in self.layers:\n",
    "            x, log_det = layer(x, context)\n",
    "            log_det_J += log_det\n",
    "        return x, log_det_J\n",
    "\n",
    "    def inverse(self, z, context):\n",
    "        log_det_J = torch.zeros(z.size(0), device=z.device)\n",
    "        for layer in reversed(self.layers):\n",
    "            z, log_det = layer.inverse(z, context)\n",
    "            log_det_J -= log_det\n",
    "        return z, log_det_J\n",
    "\n",
    "    def log_prob(self, x, context):\n",
    "        z, log_det = self.inverse(x, context)\n",
    "        return self.base_dist.log_prob(z).sum(dim=1) + log_det\n",
    "\n",
    "    def sample(self, n_samples, context):\n",
    "        z = self.base_dist.sample((n_samples, self.dim)).to(context.device)\n",
    "        x, _ = self.forward(z, context)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5badcf0-6863-4678-8c10-95ce7c38615b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log prob: tensor([-6.3100, -3.9412, -3.6735, -4.0321, -3.9567, -4.0110, -3.7756, -6.3175,\n",
      "        -4.1675, -6.0374], grad_fn=<AddBackward0>)\n",
      "torch.Size([10, 3, 3]) torch.Size([10, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 3 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m logp \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlog_prob(x, context)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLog prob:\u001b[39m\u001b[38;5;124m\"\u001b[39m, logp)\n\u001b[0;32m---> 15\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSamples:\u001b[39m\u001b[38;5;124m\"\u001b[39m, samples)\n",
      "Cell \u001b[0;32mIn[9], line 87\u001b[0m, in \u001b[0;36mRealNVP.sample\u001b[0;34m(self, n_samples, context)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, n_samples, context):\n\u001b[1;32m     86\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_dist\u001b[38;5;241m.\u001b[39msample((n_samples, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim))\u001b[38;5;241m.\u001b[39mto(context\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 87\u001b[0m     x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "Cell \u001b[0;32mIn[9], line 70\u001b[0m, in \u001b[0;36mRealNVP.forward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m     68\u001b[0m log_det_J \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 70\u001b[0m     x, log_det \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     log_det_J \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m log_det\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, log_det_J\n",
      "File \u001b[0;32m~/Desktop/Estudos/jupyterlab_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Estudos/jupyterlab_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 36\u001b[0m, in \u001b[0;36mRealNVPLayer.forward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m     34\u001b[0m x_masked \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(x_masked\u001b[38;5;241m.\u001b[39msize(), context\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m---> 36\u001b[0m x_input \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_masked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_net(x_input) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask)\n\u001b[1;32m     38\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranslate_net(x_input) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 2"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dim = 3\n",
    "    context_dim = 2\n",
    "    n_layers = 4\n",
    "\n",
    "    base_dist = torch.distributions.Normal(torch.zeros(dim), torch.ones(dim))\n",
    "    model = RealNVP(dim, n_layers, base_dist, context_dim)\n",
    "\n",
    "    x = torch.randn(10, dim)\n",
    "    context = torch.randn(10, context_dim)\n",
    "\n",
    "    logp = model.log_prob(x, context)\n",
    "    print(\"Log prob:\", logp)\n",
    "\n",
    "    samples = model.sample(10, context)\n",
    "    print(\"Samples:\", samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2333adde-0a3f-4e01-93c1-da9c068452e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proposal_logpdf(x_t, x_prev, flow):\n",
    "    flow.log_prob(x_t)\n",
    "    # finish this\n",
    "\n",
    "def vsmc_nf(y_seq, lambda_, N, T):\n",
    "    latent_dim = y_seq[0].size(0)\n",
    "    log_w = torch.zeros(T, N)\n",
    "    x_particles = torch.zeros(T, N, latent_dim)\n",
    "    ancestors = torch.zeros(T, N, dtype=torch.long)\n",
    "\n",
    "    eps = torch.randn(N, latent_dim)\n",
    "    x_t = proposal(None, eps, lambda_)\n",
    "    x_particles[0] = x_t\n",
    "    log_w[0] = (\n",
    "        prior(x_t, None) + likelihood(y_seq[0], x_t) - proposal_logpdf(x_t, None, flow)\n",
    "    ).squeeze()\n",
    "\n",
    "    for t in range(1, T):\n",
    "        w_prev = torch.softmax(log_w[t - 1], dim=0)\n",
    "        # print(w_prev)\n",
    "        a_t = torch.multinomial(w_prev, N, replacement=True)\n",
    "        ancestors[t] = a_t\n",
    "\n",
    "        x_prev = x_particles[t - 1, a_t]\n",
    "        eps = torch.randn(N, latent_dim)\n",
    "        x_t = proposal(x_prev, eps, lambda_)\n",
    "        x_particles[t] = x_t\n",
    "\n",
    "        log_w[t] = (\n",
    "            prior(x_t, x_prev) + likelihood(y_seq[t], x_t) - proposal_logpdf(x_t, x_prev, flow)\n",
    "        ).squeeze()\n",
    "\n",
    "    # Sample final trajectory\n",
    "    w_T = torch.softmax(log_w[-1], dim=0)\n",
    "    b_T = torch.multinomial(w_T, 1).item()\n",
    "    \n",
    "    # Trace back trajectory\n",
    "    x_estimates = torch.zeros_like(x_particles[:, 0])\n",
    "    # i = b_T\n",
    "    for t in reversed(range(T)):\n",
    "        x_estimates[t] = x_particles[t, b_T]\n",
    "        if t > 0:\n",
    "            i = ancestors[t, b_T]\n",
    "    \n",
    "    return log_w, x_particles, ancestors, x_estimates\n",
    "    # return log_w, x_particles\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
