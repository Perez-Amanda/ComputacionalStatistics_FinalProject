{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eecf4357-b684-4f60-8001-9885067f75dc",
   "metadata": {},
   "source": [
    "# VSMC with Normalizing Flows\n",
    "\n",
    "### To do:\n",
    "- [ ] Think about how to take into consideration `x_prev`\n",
    "- [ ] Implement it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e323981-52d3-4599-8589-cab8d2769852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import normalizing flows module\n",
    "import sys\n",
    "sys.path.append(\"./src\")\n",
    "from flows import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c94534f0-3f38-4202-9eb0-9a5996eec067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import other libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import distributions\n",
    "from torch.nn.parameter import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "314a4cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0280,  0.4885],\n",
       "        [-0.3894, -0.9720],\n",
       "        [ 1.4886,  0.6052],\n",
       "        [ 0.8581, -0.0915],\n",
       "        [-1.8067, -1.3981],\n",
       "        [-0.2618, -0.3907],\n",
       "        [ 1.0203, -0.9154],\n",
       "        [-0.7010,  1.5876]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(4, 2)\n",
    "b = torch.randn(4, 2)\n",
    "torch.cat([a, b], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a76fc41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6002, -1.4475],\n",
       "        [ 0.5411, -1.4240],\n",
       "        [ 0.1398, -0.1670],\n",
       "        [ 0.2691,  0.0152]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40f3237a-0766-4174-ab7b-b7a51430de15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal, MultivariateNormal\n",
    "\n",
    "# Classes and functions necessary for Real NVP\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    Multilayer perceptron module.\n",
    "    '''\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class RealNVPLayer(nn.Module):\n",
    "    '''\n",
    "    Real NVP layer module.\n",
    "    '''\n",
    "    def __init__(self, dim, mask, context_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Uses MLP module as scale and\n",
    "        # translate neural networks\n",
    "        self.scale_net = MLP(dim + context_dim, dim)\n",
    "        self.translate_net = MLP(dim + context_dim, dim)\n",
    "\n",
    "        # Defines mask to implement\n",
    "        # coupling layers\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        # Forward transformation\n",
    "        x_masked = x * self.mask\n",
    "        # Adding context\n",
    "        print(x_masked.shape,'\\n',context.shape)\n",
    "        x_masked_with_context = torch.cat([x_masked, context], dim=-1)\n",
    "        s = self.scale_net(x_masked_with_context) * (1 - self.mask)\n",
    "        t = self.translate_net(x_masked_with_context) * (1 - self.mask)\n",
    "        z = x_masked + (1 - self.mask) * (x * torch.exp(s) + t)\n",
    "        log_det_J = torch.sum(s, dim=1)\n",
    "        return z, log_det_J\n",
    "\n",
    "    def inverse(self, y, context):\n",
    "        # Inverse transformation\n",
    "        y_masked = y * self.mask\n",
    "        # Adding context\n",
    "        y_masked_with_context = torch.cat([y_masked, context], dim=-1)\n",
    "        s = self.scale_net(y_masked_with_context) * (1 - self.mask)\n",
    "        t = self.translate_net(y_masked_with_context) * (1 - self.mask)\n",
    "        x = y_masked + (1 - self.mask) * ((y - t) * torch.exp(-s))\n",
    "        log_det_J = torch.sum(s, dim=1)\n",
    "        return x, log_det_J\n",
    "\n",
    "\n",
    "class RealNVP(nn.Module):\n",
    "    '''\n",
    "    Real NVP module.\n",
    "    Consists of multiple Real NVP layers.\n",
    "    '''\n",
    "    def __init__(self, dim, n_layers, base_dist, context_dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.base_dist = base_dist\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(n_layers):\n",
    "            # Create masks (alternating 0s and 1s)\n",
    "            mask_list = [(i + j) % 2 for j in range(dim)]\n",
    "            mask = torch.tensor(mask_list, dtype=torch.float32)\n",
    "\n",
    "            # Add Real NVP layer\n",
    "            self.layers.append(RealNVPLayer(self.dim, mask, context_dim))\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        # Forward transformation\n",
    "        log_det_J = torch.zeros(x.size(0), device=x.device)\n",
    "        for layer in self.layers:\n",
    "            x, log_det = layer.forward(x, context)\n",
    "            log_det_J += log_det\n",
    "        return x, log_det_J\n",
    "\n",
    "    def inverse(self, z, context):\n",
    "        # Inverse transformation\n",
    "        log_det_J = torch.zeros(z.size(0), device=z.device)\n",
    "        for layer in reversed(self.layers):\n",
    "            z, log_det = layer.inverse(z, context)\n",
    "            log_det_J -= log_det\n",
    "        return z, log_det_J\n",
    "\n",
    "    def log_prob(self, x, context):\n",
    "        # Computes the log pdf of the final samples\n",
    "        z, log_det = self.inverse(x, context)\n",
    "        return self.base_dist.log_prob(z) + log_det\n",
    "        \n",
    "    def sample(self, n_samples, context):\n",
    "        # Sample from the final distribution\n",
    "        z = self.base_dist.sample((n_samples,))\n",
    "        x, _ = self.forward(z.view(n_samples, self.dim), context)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eba8588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to be used\n",
    "def prior_VSMC(x, x_prev):\n",
    "    return prior(x_prev).log_prob(x)\n",
    "\n",
    "def likelihood_VSMC(y_t, x_t):\n",
    "    return likelihood(x_t).log_prob(y_t)\n",
    "\n",
    "def proposal_logpdf(x, x_prev, lambda_):\n",
    "    if x_prev is None:\n",
    "        return Normal(lambda_, 1).log_prob(x)\n",
    "    else:\n",
    "        return Normal(lambda_ * x_prev, 1).log_prob(x)\n",
    "\n",
    "def proposal(x_prev, eps, lambda_):\n",
    "    if x_prev is None:\n",
    "        mu = lambda_\n",
    "    else:\n",
    "        mu = lambda_ * x_prev\n",
    "    return eps + mu\n",
    "\n",
    "# VSMC\n",
    "def run_vsmc(y_seq, lambda_, N, T):\n",
    "    latent_dim = y_seq[0].size(0)\n",
    "    log_w = torch.zeros(T, N)\n",
    "    x_particles = torch.zeros(T, N, latent_dim)\n",
    "    ancestors = torch.zeros(T, N, dtype=torch.long)\n",
    "\n",
    "    eps = torch.randn(N, latent_dim)\n",
    "    x_t = proposal(None, eps, lambda_)\n",
    "    x_particles[0] = x_t\n",
    "    log_w[0] = (\n",
    "        prior_VSMC(x_t, None) + likelihood_VSMC(y_seq[0], x_t) - proposal_logpdf(x_t, None, lambda_)\n",
    "    ).squeeze()\n",
    "\n",
    "    for t in range(1, T):\n",
    "        w_prev = torch.softmax(log_w[t - 1], dim=0)\n",
    "        # print(w_prev)\n",
    "        a_t = torch.multinomial(w_prev, N, replacement=True)\n",
    "        ancestors[t] = a_t\n",
    "\n",
    "        x_prev = x_particles[t - 1, a_t]\n",
    "        eps = torch.randn(N, latent_dim)\n",
    "        x_t = proposal(x_prev, eps, lambda_)\n",
    "        x_particles[t] = x_t\n",
    "\n",
    "        log_w[t] = (\n",
    "            prior_VSMC(x_t, x_prev) + likelihood_VSMC(y_seq[t], x_t) - proposal_logpdf(x_t, x_prev, lambda_)\n",
    "        ).squeeze()\n",
    "\n",
    "    # Sample final trajectory\n",
    "    w_T = torch.softmax(log_w[-1], dim=0)\n",
    "    b_T = torch.multinomial(w_T, 1).item()\n",
    "    \n",
    "    # Trace back trajectory\n",
    "    x_estimates = torch.zeros_like(x_particles[:, 0])\n",
    "    i = b_T\n",
    "    for t in reversed(range(T)):\n",
    "        x_estimates[t] = x_particles[t, i]\n",
    "        if t > 0:\n",
    "            i = ancestors[t, i]\n",
    "    \n",
    "    return log_w, x_particles, ancestors, x_estimates\n",
    "\n",
    "def surrogate_elbo(log_w):\n",
    "    logZ = torch.logsumexp(log_w, dim=1) - torch.log(torch.tensor(log_w.size(1), dtype=torch.float32))\n",
    "    return logZ.sum()\n",
    "\n",
    "def train_vsmc(y_seq, T, N, n_steps=100, lr=1e-3):\n",
    "    # Parameter to learn\n",
    "    lambda_ = torch.nn.Parameter(torch.randn((1,)))\n",
    "    optimizer = torch.optim.Adam([lambda_], lr=lr)\n",
    "    history = []\n",
    "    \n",
    "    # Training loop\n",
    "    for step in range(n_steps):\n",
    "        optimizer.zero_grad()\n",
    "        log_w,_,_,_ = run_vsmc(y_seq, lambda_, N, T=T)\n",
    "        elbo = surrogate_elbo(log_w)\n",
    "        loss = -elbo\n",
    "        loss.backward()\n",
    "        if step % 50 == 0:\n",
    "            print(f\"Step {step:03d} | ELBO: {elbo.item():.2f} | grad: {lambda_.grad.item():.4f}\")\n",
    "        optimizer.step()\n",
    "        history.append(loss)\n",
    "    print(f\"Training finished!\\nFinal loss: {loss.item()}\")\n",
    "    return history, lambda_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2575877a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0153a98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward pass\n",
      "torch.Size([5, 2]) \n",
      " torch.Size([5, 2])\n",
      "torch.Size([5, 2]) \n",
      " torch.Size([5, 2])\n",
      "torch.Size([5, 2]) \n",
      " torch.Size([5, 2])\n",
      "torch.Size([5, 2]) \n",
      " torch.Size([5, 2])\n",
      "inverse pass\n",
      "log prob\n",
      "sample\n",
      "torch.Size([5, 2]) \n",
      " torch.Size([5, 2])\n",
      "torch.Size([5, 2]) \n",
      " torch.Size([5, 2])\n",
      "torch.Size([5, 2]) \n",
      " torch.Size([5, 2])\n",
      "torch.Size([5, 2]) \n",
      " torch.Size([5, 2])\n"
     ]
    }
   ],
   "source": [
    "# Define a base distribution\n",
    "base_dist = MultivariateNormal(torch.zeros(2), torch.eye(2))\n",
    "\n",
    "# Example usage\n",
    "dim = 2  # Dimensionality of data\n",
    "n_layers = 4  # Number of RealNVP layers\n",
    "context_dim = 2  # Dimensionality of context\n",
    "\n",
    "realnvp = RealNVP(dim, n_layers, base_dist, context_dim)\n",
    "\n",
    "# Sample data\n",
    "context = torch.randn(5, context_dim)  # 10 samples, 2-dimensional context\n",
    "x = torch.randn(5, dim)  # 10 samples, 3-dimensional data\n",
    "\n",
    "# Forward pass\n",
    "print('forward pass')\n",
    "z, log_det_J = realnvp(x, context)\n",
    "\n",
    "# Inverse pass\n",
    "print('inverse pass')\n",
    "x_reconstructed, _ = realnvp.inverse(z, context)\n",
    "\n",
    "# Log probability\n",
    "print('log prob')\n",
    "log_prob = realnvp.log_prob(x, context)\n",
    "\n",
    "# Sampling from the model\n",
    "print('sample')\n",
    "samples = realnvp.sample(5, context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1da93ff4-5892-4aaf-95ea-145f9c01ddd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.4961, -2.7053, -2.0743, -2.2819, -2.2937])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultivariateNormal(torch.zeros(2), torch.eye(2)).log_prob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af45e5ef-94e0-4bce-8675-69887e451dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0368,  1.1468])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b16e5c5-2d6b-4363-b491-9356c32afe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = distributions.Normal(0, 1)\n",
    "flow = RealNVP(dim=2, n_layers=5, base_dist=normal, context_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fddca5c-0ae7-459b-b50c-a1f051ab663f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 2.1000]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = torch.tensor([[1.0, 2.1]])\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52b09292-e6a0-4a46-9a18-d27bcf36b082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6514, -0.6892]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not trained yet\n",
    "flow.log_prob(torch.tensor([[0.0]]),context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78f5ba02-cffe-4f1b-8df5-81b590c04517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to fix it (by gepeto)\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# ---------------------\n",
    "# Helper MLP class\n",
    "# ---------------------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "# RealNVP Layer\n",
    "# ---------------------\n",
    "class RealNVPLayer(nn.Module):\n",
    "    def __init__(self, dim, mask, context_dim):\n",
    "        super().__init__()\n",
    "        self.scale_net = MLP(dim + context_dim, dim)\n",
    "        self.translate_net = MLP(dim + context_dim, dim)\n",
    "        self.mask = nn.Parameter(mask, requires_grad=False)\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        x_masked = x * self.mask\n",
    "        print(x_masked.size(), context.size())\n",
    "        x_input = torch.cat([x_masked, context], dim=-1)\n",
    "        s = self.scale_net(x_input) * (1 - self.mask)\n",
    "        t = self.translate_net(x_input) * (1 - self.mask)\n",
    "        z = x_masked + (1 - self.mask) * (x * torch.exp(s) + t)\n",
    "        log_det_J = torch.sum(s, dim=1)\n",
    "        return z, log_det_J\n",
    "\n",
    "    def inverse(self, y, context):\n",
    "        y_masked = y * self.mask\n",
    "        y_input = torch.cat([y_masked, context], dim=-1)\n",
    "        s = self.scale_net(y_input) * (1 - self.mask)\n",
    "        t = self.translate_net(y_input) * (1 - self.mask)\n",
    "        x = y_masked + (1 - self.mask) * ((y - t) * torch.exp(-s))\n",
    "        log_det_J = torch.sum(s, dim=1)\n",
    "        return x, log_det_J\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "# RealNVP Model\n",
    "# ---------------------\n",
    "class RealNVP(nn.Module):\n",
    "    def __init__(self, dim, n_layers, base_dist, context_dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.base_dist = base_dist\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            mask = torch.tensor([(i + j) % 2 for j in range(dim)], dtype=torch.float32)\n",
    "            self.layers.append(RealNVPLayer(dim, mask, context_dim))\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        log_det_J = torch.zeros(x.size(0), device=x.device)\n",
    "        for layer in self.layers:\n",
    "            x, log_det = layer(x, context)\n",
    "            log_det_J += log_det\n",
    "        return x, log_det_J\n",
    "\n",
    "    def inverse(self, z, context):\n",
    "        log_det_J = torch.zeros(z.size(0), device=z.device)\n",
    "        for layer in reversed(self.layers):\n",
    "            z, log_det = layer.inverse(z, context)\n",
    "            log_det_J -= log_det\n",
    "        return z, log_det_J\n",
    "\n",
    "    def log_prob(self, x, context):\n",
    "        z, log_det = self.inverse(x, context)\n",
    "        return self.base_dist.log_prob(z).sum(dim=1) + log_det\n",
    "\n",
    "    def sample(self, n_samples, context):\n",
    "        z = self.base_dist.sample((n_samples, self.dim)).to(context.device)\n",
    "        x, _ = self.forward(z, context)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5badcf0-6863-4678-8c10-95ce7c38615b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (10) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m x = torch.randn(\u001b[32m10\u001b[39m, dim)\n\u001b[32m     10\u001b[39m context = torch.randn(\u001b[32m10\u001b[39m, context_dim)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m logp = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLog prob:\u001b[39m\u001b[33m\"\u001b[39m, logp)\n\u001b[32m     15\u001b[39m samples = model.sample(\u001b[32m10\u001b[39m, context)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 102\u001b[39m, in \u001b[36mRealNVP.log_prob\u001b[39m\u001b[34m(self, x, context)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, context):\n\u001b[32m    100\u001b[39m     \u001b[38;5;66;03m# Computes the log pdf of the final samples\u001b[39;00m\n\u001b[32m    101\u001b[39m     z, log_det = \u001b[38;5;28mself\u001b[39m.inverse(x, context)\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_dist\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_det\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (3) must match the size of tensor b (10) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dim = 3\n",
    "    context_dim = 2\n",
    "    n_layers = 4\n",
    "\n",
    "    base_dist = torch.distributions.Normal(torch.zeros(dim), torch.ones(dim))\n",
    "    model = RealNVP(dim, n_layers, base_dist, context_dim)\n",
    "\n",
    "    x = torch.randn(10, dim)\n",
    "    context = torch.randn(10, context_dim)\n",
    "\n",
    "    logp = model.log_prob(x, context)\n",
    "    print(\"Log prob:\", logp)\n",
    "\n",
    "    samples = model.sample(10, context)\n",
    "    print(\"Samples:\", samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2333adde-0a3f-4e01-93c1-da9c068452e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proposal_logpdf(x_t, x_prev, flow):\n",
    "    flow.log_prob(x_t)\n",
    "    # finish this\n",
    "\n",
    "def vsmc_nf(y_seq, lambda_, N, T):\n",
    "    latent_dim = y_seq[0].size(0)\n",
    "    log_w = torch.zeros(T, N)\n",
    "    x_particles = torch.zeros(T, N, latent_dim)\n",
    "    ancestors = torch.zeros(T, N, dtype=torch.long)\n",
    "\n",
    "    eps = torch.randn(N, latent_dim)\n",
    "    x_t = proposal(None, eps, lambda_)\n",
    "    x_particles[0] = x_t\n",
    "    log_w[0] = (\n",
    "        prior(x_t, None) + likelihood(y_seq[0], x_t) - proposal_logpdf(x_t, None, flow)\n",
    "    ).squeeze()\n",
    "\n",
    "    for t in range(1, T):\n",
    "        w_prev = torch.softmax(log_w[t - 1], dim=0)\n",
    "        # print(w_prev)\n",
    "        a_t = torch.multinomial(w_prev, N, replacement=True)\n",
    "        ancestors[t] = a_t\n",
    "\n",
    "        x_prev = x_particles[t - 1, a_t]\n",
    "        eps = torch.randn(N, latent_dim)\n",
    "        x_t = proposal(x_prev, eps, lambda_)\n",
    "        x_particles[t] = x_t\n",
    "\n",
    "        log_w[t] = (\n",
    "            prior(x_t, x_prev) + likelihood(y_seq[t], x_t) - proposal_logpdf(x_t, x_prev, flow)\n",
    "        ).squeeze()\n",
    "\n",
    "    # Sample final trajectory\n",
    "    w_T = torch.softmax(log_w[-1], dim=0)\n",
    "    b_T = torch.multinomial(w_T, 1).item()\n",
    "    \n",
    "    # Trace back trajectory\n",
    "    x_estimates = torch.zeros_like(x_particles[:, 0])\n",
    "    # i = b_T\n",
    "    for t in reversed(range(T)):\n",
    "        x_estimates[t] = x_particles[t, b_T]\n",
    "        if t > 0:\n",
    "            i = ancestors[t, b_T]\n",
    "    \n",
    "    return log_w, x_particles, ancestors, x_estimates\n",
    "    # return log_w, x_particles\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f2bc4c",
   "metadata": {},
   "source": [
    "função run_vsmc-> olhar pra ela e tudo que ta na mesma celula. tem que fazer o treinamento dela mas a proposta deve ser substituida pela distribuição aprendida pelo normalizing flow --- pedir pro manus se sobrar processamento\n",
    "\n",
    "também é interessante configurar github nesse compiuter pra subir as coisas\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
